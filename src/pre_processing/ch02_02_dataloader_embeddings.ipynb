{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Chapter 2, part 2 - Data loader for pretrain LLM\n",
    "In the previous part we implemented tokenizers to transform raw text into integer token IDs for further processing, in this part we look into how to load torch dataset from a text and how to generate self-labeling data loaders. \n"
   ],
   "id": "88bf0533550badc8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T04:13:43.114302Z",
     "start_time": "2024-06-24T04:13:43.108187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# let's use tiktoken as tokenizer\n",
    "import tiktoken\n",
    "from importlib.metadata import version\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n"
   ],
   "id": "378bf238e98e73c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.7.0\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T04:13:43.120580Z",
     "start_time": "2024-06-24T04:13:43.115459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# open the text and transfer it to token IDs\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ],
   "id": "44b6582396fe74f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5146\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T04:13:43.123293Z",
     "start_time": "2024-06-24T04:13:43.121253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# remove the first 50 tokens as they are not as interesting as the following part?\n",
    "enc_sample = enc_text[50:]\n",
    "print(len(enc_sample))"
   ],
   "id": "4d4e824d6cae1b63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5096\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "One of the easiest and most intuitive ways to create the input-target pairs for the next-world prediction task is to create two variables, x and y, where x contains the input tokens and y contains the targets, which are the inputs shifted by 1:",
   "id": "e73044b5a21baec6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T04:13:43.127464Z",
     "start_time": "2024-06-24T04:13:43.124607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")\n",
    "\n",
    "# now let's visualize one training datum\n",
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ],
   "id": "38a57218b8d6edc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n",
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A PyTorch Dataset class and a DataLoader method to load training data",
   "id": "e73b00b7e3babfb0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T04:13:43.132127Z",
     "start_time": "2024-06-24T04:13:43.128153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        \n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        # convert all string txt to token ids\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        \n",
    "        # here max_length is the length of the sampling window, it's the same as context_size in the previous cell\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i: i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "    \n",
    "\n",
    "def create_dataloader_v1(txt,\n",
    "                         batch_size=4,\n",
    "                         max_length=256,\n",
    "                         stride=128,\n",
    "                         shuffle=True,\n",
    "                         drop_last = True,\n",
    "                         num_workers=0) -> DataLoader:\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ],
   "id": "a2466023dd9fa721",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's use the above code",
   "id": "1d25d0431e3602b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T04:13:43.162570Z",
     "start_time": "2024-06-24T04:13:43.133039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "# visualize the 1st batch\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ],
   "id": "6e19ce2639859140",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Chapter 2, part 3 - Token embeddings\n",
    "In the previous part we implemented PyTorch dataset and dataloader to load raw text data, convert them into token IDs, sample input & target tensors based on batch size & sliding window size.\n",
    "\n",
    "After the above steps, now we can get batches of training samples from the dataloader which are in the discrete integer format. To train a model we need to do further processing to make discrete token ID values to a continuous-floating-value tensor which can be used to train a tensor deep learning model.\n",
    "\n",
    "Embedding is also a learnable layer, so we first initialize with random values and update its weights during training.   "
   ],
   "id": "805dd084b6d4b1d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### An example\n",
    "reference - torch.nn.Embedding: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "\n",
    "suppose we have a vocabulary with size **V**, and we want the embedding vector to have a dimension of **E**\n",
    "\n",
    "An embedding layer is like a look-up mapping from a word (an index in the vocabulary) to a vendor with dimension E. so we can easily imagine the embedding layer can be in the shape of **[V, E]**\n",
    "\n",
    "suppose for a small example where V=6 and E=3:"
   ],
   "id": "1c0939a83e10e098"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T04:13:56.408463Z",
     "start_time": "2024-06-24T04:13:56.397177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)\n",
    "\n",
    "# to get the embedding of a word at with tokenId 3\n",
    "print(embedding_layer(torch.tensor([3])))\n",
    "\n",
    "# to get a batch of embeddings of a group of token IDs\n",
    "input_ids = torch.tensor([2,3,5,1])\n",
    "print(embedding_layer(input_ids))"
   ],
   "id": "4f409aee1e82d616",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Encoding word positions\n",
    "\n",
    "In the above embedding mechanism, we introduced a learnable mapping between integer tokenIDs and continuous double-value embeddings. However, in position wise, this embedding is static, meaning if the same word appears in different positions, in language context they represent different information but if only the above embedding is used they will have the same embedding values, making it hard for LLM/attention to learn the context.  "
   ],
   "id": "73e18f262cee0a0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T04:30:30.626484Z",
     "start_time": "2024-06-24T04:30:30.473496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embeding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "# to get a batch size of 8, 4 token each as the window size for training, we will get a 8x4x256 tensor\n",
    "\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, \n",
    "                                  batch_size=8, \n",
    "                                  max_length=max_length, \n",
    "                                  stride=max_length, \n",
    "                                  shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\",inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ],
   "id": "145c7fc86c21fe7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T04:31:06.272019Z",
     "start_time": "2024-06-24T04:31:06.269223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token_embeddings = token_embeding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ],
   "id": "7269c6d9b07f09ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T04:32:18.616710Z",
     "start_time": "2024-06-24T04:32:18.611653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# for positional embedding\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ],
   "id": "d53ca4cc6325a86e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T04:33:19.044357Z",
     "start_time": "2024-06-24T04:33:19.041048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# final input embeddings are the sum of original embeddings and positional embeddings\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ],
   "id": "f7bb37ae9f264e53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Chapter 2 summary\n",
    "1. LLMs require textual data to be converted into numerical vectors, known as embeddings since they can’t process raw text. Embeddings transform discrete data (like words or images) into continuous vector spaces, making them compatible with neural network operations.\n",
    "\n",
    "2. As the first step, raw text is broken into tokens, which can be words or characters. Then, the tokens are converted into integer representations, termed token IDs.\n",
    "\n",
    "3. Special tokens, such as <|unk|> and <|endoftext|>, can be added to enhance the model’s understanding and handle various contexts, such as unknown words or marking the boundary between unrelated texts.\n",
    "\n",
    "4. The byte pair encoding (BPE) tokenizer used for LLMs like GPT-2 and GPT-3 can efficiently handle unknown words by breaking them down into subword units or individual characters.\n",
    "\n",
    "5. We use a sliding window approach on tokenized data to generate input-target pairs for LLM training.\n",
    "\n",
    "6. Embedding layers in PyTorch function as a lookup operation, retrieving vectors corresponding to token IDs. The resulting embedding vectors provide continuous representations of tokens, which is crucial for training deep learning models like LLMs.\n",
    "\n",
    "7. While token embeddings provide consistent vector representations for each token, they lack a sense of the token’s position in a sequence. To rectify this, two main types of positional embeddings exist: absolute and relative. OpenAI’s GPT models utilize absolute positional embeddings that are added to the token embedding vectors and are optimized during the model training."
   ],
   "id": "287c85c1451099cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e8cdc9f0ca0d7306"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
