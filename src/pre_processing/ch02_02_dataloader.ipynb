{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Chapter 2, part 2 - Data loader for pretrain LLM\n",
    "In the previous part we implemented tokenizers to transform raw text into integer token IDs for further processing, in this part we look into how to load torch dataset from a text and how to generate self-labeling data loaders. \n"
   ],
   "id": "88bf0533550badc8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T04:36:46.187392Z",
     "start_time": "2024-06-11T04:36:46.181837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# let's use tiktoken as tokenizer\n",
    "import tiktoken\n",
    "from importlib.metadata import version\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n"
   ],
   "id": "378bf238e98e73c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.7.0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T04:38:21.278162Z",
     "start_time": "2024-06-11T04:38:21.268786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# open the text and transfer it to token IDs\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ],
   "id": "44b6582396fe74f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5146\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T04:49:02.718817Z",
     "start_time": "2024-06-11T04:49:02.715508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# remove the first 50 tokens as they are not as interesting as the following part?\n",
    "enc_sample = enc_text[50:]\n",
    "print(len(enc_sample))"
   ],
   "id": "4d4e824d6cae1b63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5096\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "One of the easiest and most intuitive ways to create the input-target pairs for the next-world prediction task is to create two variables, x and y, where x contains the input tokens and y contains the targets, which are the inputs shifted by 1:",
   "id": "e73044b5a21baec6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T04:20:48.135727Z",
     "start_time": "2024-06-12T04:20:48.131243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")\n",
    "\n",
    "# now let's visualize one training datum\n",
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ],
   "id": "38a57218b8d6edc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n",
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A PyTorch Dataset class and a DataLoader method to load training data",
   "id": "e73b00b7e3babfb0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T04:41:51.542903Z",
     "start_time": "2024-06-12T04:41:51.537141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        \n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        # convert all string txt to token ids\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        \n",
    "        # here max_length is the length of the sampling window, it's the same as context_size in the previous cell\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i: i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "    \n",
    "\n",
    "def create_dataloader_v1(txt,\n",
    "                         batch_size=4,\n",
    "                         max_length=256,\n",
    "                         stride=128,\n",
    "                         shuffle=True,\n",
    "                         drop_last = True,\n",
    "                         num_workers=0) -> DataLoader:\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ],
   "id": "a2466023dd9fa721",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's use the above code",
   "id": "1d25d0431e3602b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T04:45:46.643193Z",
     "start_time": "2024-06-12T04:45:46.564348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "# visualize the 1st batch\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ],
   "id": "6e19ce2639859140",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bcb1c2046631c120"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
