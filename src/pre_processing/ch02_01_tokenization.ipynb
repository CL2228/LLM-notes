{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Chapter 2, part 1 - Tokenization for input text\n",
    "To pretrain a LLM we use a huge amount of text to train the model for it to gain context between words. \n",
    "The first step to prepare training data is to preprocess the raw input text. This notebook covers steps on how to transfer raw text in String format to integer formate as numeric format makes more sense for Python/PyTorch to process.\n"
   ],
   "id": "7753d41288c4ff98"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-06T20:26:41.077990Z",
     "start_time": "2024-06-06T20:26:41.075644Z"
    }
   },
   "source": "print(\"hello world\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### preprocess the raw text to individual words (including punctuations)\n",
    "after preprocessing the whole raw text file becomes individual tokens."
   ],
   "id": "aa8d6a86480d6943"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T20:26:41.115856Z",
     "start_time": "2024-06-06T20:26:41.113038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we should have the the-verdict.txt file ready in local env.\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# inspect the length of words and print a sample\n",
    "print(len(raw_text))\n",
    "print(raw_text[:99])"
   ],
   "id": "f751808d5e716a94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20480\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T20:26:41.132624Z",
     "start_time": "2024-06-06T20:26:41.129452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# sample code to use regular expression to tokenize an input text stream\n",
    "import re\n",
    "sample_text = \"Hello, word. This, is a test.\"\n",
    "\n",
    "# this way we are splitting based on spaces, not ideal because there are punctuation characters attached to words\n",
    "result = re.split(r'(\\s)', sample_text)\n",
    "print(result)\n",
    "\n",
    "# this way we are splitting on whitespaces (\\s), commas, and periods, it's still not ideal because an empty string or a whitespace is an element\n",
    "result = re.split(r'([,.]|\\s)', sample_text)\n",
    "print(result)\n",
    "# we can get rid of spaces with:\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)\n",
    "\n",
    "# for our short story text we want to also include text like \"--\" when we do tokenization, so we can:\n",
    "sample_text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', sample_text)\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ],
   "id": "b8ca11f4d17b9157",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'word.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n",
      "['Hello', ',', '', ' ', 'word', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n",
      "['Hello', ',', 'word', '.', 'This', ',', 'is', 'a', 'test', '.']\n",
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T20:26:41.136982Z",
     "start_time": "2024-06-06T20:26:41.133537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# let's use this RE scheme on the input text\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n",
    "\n",
    "# let's inspect 30 elements, looks pretty good\n",
    "print(preprocessed[:30])"
   ],
   "id": "a84a6ba9298f0487",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Convert tokens into token IDs\n",
    "tokens are still in String type, now we need to map tokens into integers that can be further processed by Python.\n",
    "\n",
    "steps to converting to token IDs:\n",
    "1. We first get a set of unique words from tokens.\n",
    "2. We sort the set alphabetically, and label them from 0 to N (N is the number of unique tokens)\n",
    "3. With the labels we map words into integer token IDs.\n"
   ],
   "id": "bbfeb39e5c7a0e71"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T20:26:41.140708Z",
     "start_time": "2024-06-06T20:26:41.137772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_unique_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_unique_words)\n",
    "print(\"total unique tokens:\", vocab_size)\n",
    "\n",
    "# let's check some token IDs\n",
    "vocab = {token: integer for integer, token in enumerate(all_unique_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 20:\n",
    "        break"
   ],
   "id": "32fb6393962c6423",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total unique tokens: 1130\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T20:26:41.145431Z",
     "start_time": "2024-06-06T20:26:41.141736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# in the above cell we have figured out how to get a mapping from token (String) to token ID (integer), now let's wrap it up in a class for token encoding & decoding.\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocal: dict):\n",
    "        # in constructor, we initiate bidirectional mapping\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {token_id: token for token, token_id in vocab.items()}\n",
    "        \n",
    "    # this is an encoding method used to transform text input to a series of token IDs\n",
    "    def encode(self, text: str) -> list[int]: \n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    # this is a decoding method used to transform token IDs back to the string text\n",
    "    def decode(self, ids: list[int]) -> str:\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        \n",
    "        # this step is to remove spaces before the specified punctuation\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)     \n",
    "        return text\n",
    "    \n",
    "\n",
    "# now let's use this SimpleTokenizerV1 class to do some testing\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "# encode into IDs\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "# decode back to string\n",
    "print(tokenizer.decode(ids))\n",
    "        "
   ],
   "id": "5e5b99a76619ab0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the above class we successfully built a tokenizer which can do encoding and decoding\n",
    "but if we fed an unknown word to the tokenizer, it will throw an error. "
   ],
   "id": "1f7fc37f77927bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T20:26:41.182670Z",
     "start_time": "2024-06-06T20:26:41.168739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ],
   "id": "807d9ab172a11da0",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHello, do you like tea?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m)\n",
      "Cell \u001B[0;32mIn[29], line 12\u001B[0m, in \u001B[0;36mSimpleTokenizerV1.encode\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m     10\u001B[0m preprocessed \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m([,.:;?_!\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m()\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124m]|--|\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms)\u001B[39m\u001B[38;5;124m'\u001B[39m, text)\n\u001B[1;32m     11\u001B[0m preprocessed \u001B[38;5;241m=\u001B[39m [item\u001B[38;5;241m.\u001B[39mstrip() \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m preprocessed \u001B[38;5;28;01mif\u001B[39;00m item\u001B[38;5;241m.\u001B[39mstrip()]\n\u001B[0;32m---> 12\u001B[0m ids \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstr_to_int[s] \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m preprocessed]\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ids\n",
      "Cell \u001B[0;32mIn[29], line 12\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     10\u001B[0m preprocessed \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m([,.:;?_!\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m()\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124m]|--|\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms)\u001B[39m\u001B[38;5;124m'\u001B[39m, text)\n\u001B[1;32m     11\u001B[0m preprocessed \u001B[38;5;241m=\u001B[39m [item\u001B[38;5;241m.\u001B[39mstrip() \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m preprocessed \u001B[38;5;28;01mif\u001B[39;00m item\u001B[38;5;241m.\u001B[39mstrip()]\n\u001B[0;32m---> 12\u001B[0m ids \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstr_to_int\u001B[49m\u001B[43m[\u001B[49m\u001B[43ms\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m preprocessed]\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ids\n",
      "\u001B[0;31mKeyError\u001B[0m: 'Hello'"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In addition to all the words that we feed into vocab construction, we also need more additional contextual tokens which can enhance models' understanding. \n",
    "Here we are adding two context tokens - `<|unk|>` and `<|endoftext|>` to represent unknown and endOfText respectively. "
   ],
   "id": "386841e1a848bdcb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T20:36:11.675975Z",
     "start_time": "2024-06-06T20:36:11.671407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_unique_words = sorted(list(set(preprocessed)))\n",
    "all_unique_words.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token: integer for integer, token in enumerate(all_unique_words)}\n",
    "print(len(vocab.items()))\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ],
   "id": "5cfd49fb0941e21d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n",
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T20:42:21.728808Z",
     "start_time": "2024-06-06T20:42:21.722290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# after adding the above two special contextual tokens, let's construct a tokenizer V2\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocal: dict):\n",
    "        # in constructor, we initiate bidirectional mapping\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {token_id: token for token, token_id in vocab.items()}\n",
    "        \n",
    "    # this is an encoding method used to transform text input to a series of token IDs\n",
    "    def encode(self, text: str) -> list[int]: \n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    # this is a decoding method used to transform token IDs back to the string text\n",
    "    def decode(self, ids: list[int]) -> str:\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        \n",
    "        # this step is to remove spaces before the specified punctuation\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)     \n",
    "        return text\n",
    "    \n",
    "# let's test using two unrelated sentences\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join([text1, text2])\n",
    "print(text)\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ],
   "id": "72569256c9de6ffa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Byte pare encoding (BPE)\n",
    "[BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding) is a sophisticated way to handle unknown word situations.\n",
    "An example of BPE:\n",
    "\n",
    "Suppose the data to be encoded is\n",
    "```\n",
    "aaabdaaabac\n",
    "```\n",
    "The byte pair \"aa\" occurs most often, so it will be replaced by a byte that is not used in the data, such as \"Z\". Now there is the following data and replacement table:\n",
    "```\n",
    "ZabdZabac\n",
    "Z=aa\n",
    "```\n",
    "Then the process is repeated with byte pair \"ab\", replacing it with \"Y\":\n",
    "```\n",
    "ZYdZYac\n",
    "Y=ab\n",
    "Z=aa\n",
    "```\n",
    "The only literal byte pair left occurs only once, and the encoding might stop here. Alternatively, the process could continue with recursive byte pair encoding, replacing \"ZY\" with \"X\":\n",
    "```\n",
    "XdXac\n",
    "X=ZY\n",
    "Y=ab\n",
    "Z=aa\n",
    "```\n",
    "This data cannot be compressed further by byte pair encoding because there are no pairs of bytes that occur more than once.\n",
    "To decompress the data, simply perform the replacements in the reverse order.\n",
    "\n",
    "\n",
    "**It is complicated to implement BPE from scratch, so here we are using openai/tiktoken library**"
   ],
   "id": "e404e9794fb796ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T04:53:48.216668Z",
     "start_time": "2024-06-07T04:53:48.207179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import & check the version\n",
    "import tiktoken\n",
    "from importlib.metadata import version\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ],
   "id": "94bbdad0340ec3c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.7.0\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T05:02:41.287540Z",
     "start_time": "2024-06-07T05:02:41.282955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = (\n",
    "\"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "\"of someunknownPlace.\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)\n",
    "\n",
    "# try out unknown word scenario\n",
    "text = \"Akwirw ier\"\n",
    "integers = tokenizer.encode(text)\n",
    "print(integers)\n",
    "strings = [tokenizer.decode([element]) for element in integers]\n",
    "print(strings)"
   ],
   "id": "c0c87caf1ff69d31",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n",
      "[33901, 86, 343, 86, 220, 959]\n",
      "['Ak', 'w', 'ir', 'w', ' ', 'ier']\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3b8284169226c024"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
