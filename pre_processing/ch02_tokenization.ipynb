{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# This is the notebook for chapter 2 - Tokenization part",
   "id": "7753d41288c4ff98"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-06T05:46:16.927968Z",
     "start_time": "2024-06-06T05:46:16.923060Z"
    }
   },
   "source": "print(\"hello world\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### preprocess the raw text to individual words (including punctuations)\n",
    "after preprocessing the whole raw text file becomes individual tokens."
   ],
   "id": "aa8d6a86480d6943"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T05:46:16.931776Z",
     "start_time": "2024-06-06T05:46:16.929190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we should have the the-verdict.txt file ready in local env.\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# inspect the length of words and print a sample\n",
    "print(len(raw_text))\n",
    "print(raw_text[:99])"
   ],
   "id": "f751808d5e716a94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20480\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T05:46:16.935692Z",
     "start_time": "2024-06-06T05:46:16.932549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# sample code to use regular expression to tokenize an input text stream\n",
    "import re\n",
    "sample_text = \"Hello, word. This, is a test.\"\n",
    "\n",
    "# this way we are splitting based on spaces, not ideal because there are punctuation characters attached to words\n",
    "result = re.split(r'(\\s)', sample_text)\n",
    "print(result)\n",
    "\n",
    "# this way we are splitting on whitespaces (\\s), commas, and periods, it's still not ideal because an empty string or a whitespace is an element\n",
    "result = re.split(r'([,.]|\\s)', sample_text)\n",
    "print(result)\n",
    "# we can get rid of spaces with:\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)\n",
    "\n",
    "# for our short story text we want to also include text like \"--\" when we do tokenization, so we can:\n",
    "sample_text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', sample_text)\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ],
   "id": "b8ca11f4d17b9157",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'word.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n",
      "['Hello', ',', '', ' ', 'word', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n",
      "['Hello', ',', 'word', '.', 'This', ',', 'is', 'a', 'test', '.']\n",
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T05:46:16.939806Z",
     "start_time": "2024-06-06T05:46:16.936476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# let's use this RE scheme on the input text\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n",
    "\n",
    "# let's inspect 30 elements, looks pretty good\n",
    "print(preprocessed[:30])"
   ],
   "id": "a84a6ba9298f0487",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Convert tokens into token IDs\n",
    "tokens are still in String type, now we need to map tokens into integers that can be further processed by Python.\n",
    "\n",
    "steps to converting to token IDs:\n",
    "1. We first get a set of unique words from tokens.\n",
    "2. We sort the set alphabetically, and label them from 0 to N (N is the number of unique tokens)\n",
    "3. With the labels we map words into integer token IDs.\n"
   ],
   "id": "bbfeb39e5c7a0e71"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T05:55:23.270536Z",
     "start_time": "2024-06-06T05:55:23.265802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_unique_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_unique_words)\n",
    "print(\"total unique tokens:\", vocab_size)\n",
    "\n",
    "# let's check some token IDs\n",
    "vocab = {token: integer for integer, token in enumerate(all_unique_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ],
   "id": "32fb6393962c6423",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total unique tokens: 1130\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5e5b99a76619ab0c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
